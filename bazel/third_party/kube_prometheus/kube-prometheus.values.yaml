namespaceOverride: grafana-stack

# Note: We delete all of the kube-prometheus ServiceMonitors that correspond to pods that are
# injected with the Istio sidecar. There is no need to define ServiceMonitors for these service
# as the general Istio PodMonitor will scrape their metrics. The ServiceMonitors that remain
# are for services that aren't injected - sometimes this is due to the pods being in the
# kube-system namespace which isn't injected by default and sometimes it's due to pods like
# the node exporter which share the host's network namespace and aren't injected. The other
# option is that we just provision all of the ServiceMonitors for the kube-system and
# grafana-stack namespaces and just disabled Istio sidecar injection for the entire
# grafana-stack namespace - we might go down this path but unsure of implications.

prometheusOperator:
  # This reuses the secret generated by kube-webhook-certgen below (that we've disabled). If
  # we don't also disable TLS here then the prometheus-operator pod will fail to start since
  # that secret won't exist.
  tls:
    enabled: false

  admissionWebhooks:
    # When enabled, the kube-webhook-certgen project is used to generate a self-signed cert
    # into a Kubernetes secret and then patch the admission controller configuration so that
    # it uses the cert. The problem is that all published image versions of kube-webhook-certgen
    # use the v1beta1 API group of MutatingWebhookConfiguration rather than v1 which is now
    # standard. So disabling for now and will replace with cert-manager later.
    enabled: false

  # Metrics scaping is handled by Istio PodMonitor.
  serviceMonitor:
    selfMonitor: false

  # Enable aggregation of Istio and application metrics.
  podAnnotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
    prometheus.io/scrape: "true"

# This section configures the Prometheus CR that provisions the central Prometheus instance.
prometheus:
  prometheusSpec:
    externalUrl: http://office.00rg.com:8082/prometheus
    enableRemoteWriteReceiver: true

    # By default, don't use the prometheus-operator to manage ServiceMonitors, PodMonitors or
    # Probes as we'll use grafana-agent-operator to manage these instead.
    serviceMonitorSelector:
      matchLabels:
        reconcile-with-prometheus-operator: "true"
    podMonitorSelector:
      matchLabels:
        reconcile-with-prometheus-operator: "true"
    probeSelector:
      matchLabels:
        reconcile-with-prometheus-operator: "true"

    # Pick up all PrometheusRule resources.
    ruleSelector:
      matchLabels: {}

    # Don't inject the Prometheus instance with the Istio sidecar. While this does seem to work,
    # I've seen quite a bit of advice against it. E.g., in the Istio documentation here
    # https://istio.io/latest/docs/ops/integrations/prometheus/#tls-settings they mention:
    #
    #    However, the sidecar should not intercept requests for Prometheus because Prometheus’s
    #    model of direct endpoint access is incompatible with Istio’s sidecar proxy model.
    #
    # And in https://superorbital.io/journal/istio-metrics-merging they mention:
    #
    #    Note: while you can still find people suggesting the use of an Istio sidecar being added
    #    to Prometheus to enable mTLS scraping, that approach is no longer recommended or necessary,
    #    do not do this! (read on for a full solution).
    #
    # Note also that we allow the ServiceMonitor for the Prometheus instance to be generated
    # (via grafana-stack/base/kustomization.yaml). Since the Prometheus pods aren't injected with
    # the Istio sidecar, we can't rely on the general Istio PodMonitor to scrape Prometheus itself.
    podMetadata:
      annotations:
        sidecar.istio.io/inject: "false"

alertmanager:
  # Metrics scaping is handled by Istio PodMonitor.
  serviceMonitor:
    selfMonitor: false

  # Note: to manage the Alertmanager config properly as a secret you need to use the
  # alertmanagerSpec field to map in the config as a volume rather than it being specified
  # verbatim in the config field. Could probably use a standard Kubernetes secret or the
  # Secret Store CSI Driver and different secret backends depending on whether you're on
  # the cloud (e.g. https://github.com/aws/secrets-store-csi-driver-provider-aws) or running
  # locally (e.g. https://github.com/hashicorp/vault-csi-provider). Would be ideal to be able
  # to specify the non-secret parts of the config as raw YAML in this repo (see
  # https://github.com/machinezone/configmapsecrets).
  alertmanagerSpec:
    externalUrl: http://office.00rg.com:8082/alertmanager

    # Enable aggregation of Istio and application metrics.
    podMetadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9093"
        prometheus.io/scrape: "true"

  # Alertmanager configuration. See:
  # - https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/alerting.md
  # - https://prometheus.io/docs/alerting/latest/configuration/
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      routes:
      - match:
          alertname: InstanceLowMemory
          severity: critical
        receiver: "null"
      - match:
        receiver: pagerduty
        continue: true
    receivers:
    - name: "null"
    - name: pagerduty

grafana:
  namespaceOverride: grafana-stack
  # Disable the Grafana chart tests as because we're not using Helm to do the deployment the
  # tests will start running immediately and then fail.
  testFramework:
    enabled: false

  # Disable ServiceMonitor as we use the Istio PodMonitor instead.
  serviceMonitor:
    enabled: false

  # Enable aggregation of Istio and application metrics.
  podAnnotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "3000"
    prometheus.io/scrape: "true"

  grafana.ini:
    server:
      root_url: http://office.00rg.com:8082/grafana
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Loki
        type: loki
        url: http://loki:3100

  dashboardProviders:
   dashboardproviders.yaml:
     apiVersion: 1
     providers:
     - name: default
       type: file
       disableDeletion: true
       editable: false
       options:
         path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      # Istio Control Plane Dashboard for Istio 1.16.1
      istio-control-plane-dashboard:
        gnetId: 7645
        revision: 146
        datasource: Prometheus
      # Istio Mesh Dashboard for Istio 1.16.1
      istio-mesh-dashboard:
        gnetId: 7639
        revision: 146
        datasource: Prometheus
      # Istio Service Dashboard for Istio 1.16.1
      istio-service-dashboard:
        gnetId: 7636
        revision: 146
        datasource: Prometheus
      # Istio Workload Dashboard for Istio 1.16.1
      istio-workload-dashboard:
        gnetId: 7630
        revision: 146
        datasource: Prometheus
      # Istio Performance Dashboard for Istio 1.16.1
      istio-performance-dashboard:
        gnetId: 11829
        revision: 146
        datasource: Prometheus
      # Istio WASM Extension Dashboard for Istio 1.16.1
      istio-wasm-dashboard:
        gnetId: 13277
        revision: 103
        datasource: Prometheus

kube-state-metrics:
  namespaceOverride: grafana-stack

  # Disable ServiceMonitor as we use the Istio PodMonitor instead.
  prometheus:
    monitor:
      enabled: false

  # Enable aggregation of Istio and application metrics.
  podAnnotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
    prometheus.io/scrape: "true"

prometheus-node-exporter:
  namespaceOverride: grafana-stack
