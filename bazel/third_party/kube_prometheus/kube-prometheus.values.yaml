namespaceOverride: grafana-stack

prometheusOperator:
  # This reuses the secret generated by kube-webhook-certgen below (that we've disabled). If
  # we don't also disable TLS here then the prometheus-operator pod will fail to start since
  # that secret won't exist.
  tls:
    enabled: false

  admissionWebhooks:
    # When enabled, the kube-webhook-certgen project is used to generate a self-signed cert
    # into a Kubernetes secret and then patch the admission controller configuration so that
    # it uses the cert. The problem is that all published image versions of kube-webhook-certgen
    # use the v1beta1 API group of MutatingWebhookConfiguration rather than v1 which is now
    # standard. So disabling for now and will replace with cert-manager later.
    enabled: false

# This section configures the Prometheus CR that provisions the central Prometheus instance.
prometheus:
  prometheusSpec:
    externalUrl: http://office.borgorg.com:8082/prometheus
    enableRemoteWriteReceiver: true

    # By default, don't use the prometheus-operator to manage ServiceMonitors, PodMonitors or
    # Probes as we'll use grafana-agent-operator to manage these instead.
    serviceMonitorSelector:
      matchLabels:
        reconcile-with-prometheus-operator: "true"
    podMonitorSelector:
      matchLabels:
        reconcile-with-prometheus-operator: "true"
    probeSelector:
      matchLabels:
        reconcile-with-prometheus-operator: "true"

    # Pick up all PrometheusRule resources.
    ruleSelector:
      matchLabels: {}

alertmanager:
  # Note: to manage the Alertmanager config properly as a secret you need to use the
  # alertmanagerSpec field to map in the config as a volume rather than it being specified
  # verbatim in the config field. Could probably use a standard Kubernetes secret or the
  # Secret Store CSI Driver and different secret backends depending on whether you're on
  # the cloud (e.g. https://github.com/aws/secrets-store-csi-driver-provider-aws) or running
  # locally (e.g. https://github.com/hashicorp/vault-csi-provider). Would be ideal to be able
  # to specify the non-secret parts of the config as raw YAML in this repo (see
  # https://github.com/machinezone/configmapsecrets).
  alertmanagerSpec:
    externalUrl: http://office.borgorg.com:8082/alertmanager

  # Alertmanager configuration. See:
  # - https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/alerting.md
  # - https://prometheus.io/docs/alerting/latest/configuration/
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      routes:
      - match:
          alertname: InstanceLowMemory
          severity: critical
        receiver: "null"
      - match:
        receiver: pagerduty
        continue: true
    receivers:
    - name: "null"
    - name: pagerduty

grafana:
  namespaceOverride: grafana-stack
  # Disable the Grafana chart tests as because we're not using Helm to do the deployment the
  # tests will start running immediately and then fail.
  testFramework:
    enabled: false

  grafana.ini:
    server:
      root_url: http://office.borgorg.com:8082/grafana

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Loki
        type: loki
        url: http://loki:3100
      - name: Tempo
        type: tempo
        url: http://tempo:3100
        # TODO: This needs more changes to work properly.
        jsonData:
          httpMethod: GET
          serviceMap:
            datasourceUid: prometheus

  dashboardProviders:
   dashboardproviders.yaml:
     apiVersion: 1
     providers:
     - name: default
       type: file
       disableDeletion: true
       editable: false
       options:
         path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      # Community Kubernetes dashboards from https://grafana.com/orgs/imrtfm.
      # See also: https://github.com/dotdc/grafana-dashboards-kubernetes
      # And: https://grafana.com/blog/2023/01/06/how-to-monitor-kubernetes-with-grafana-and-prometheus-inside-powders-observability-stack
      kubernetes-namespaces-dashboard:
        gnetId: 15758
        revision: 15
        datasource: Prometheus
      kubernetes-global-dashboard:
        gnetId: 15757
        revision: 22
        datasource: Prometheus
      kubernetes-pods-dashboard:
        gnetId: 15760
        revision: 15
        datasource: Prometheus
      kubernetes-nodes-dashboard:
        gnetId: 15759
        revision: 14
        datasource: Prometheus
      kubernetes-apiserver-dashboard:
        gnetId: 15761
        revision: 11
        datasource: Prometheus
      kubernetes-coredns-dashboard:
        gnetId: 15762
        revision: 10
        datasource: Prometheus

      # Istio community dashboards from https://grafana.com/orgs/istio.
      # All dashboard revisions below are for Istio 1.16.1.
      istio-control-plane-dashboard:
        gnetId: 7645
        revision: 146
        datasource: Prometheus
      istio-mesh-dashboard:
        gnetId: 7639
        revision: 146
        datasource: Prometheus
      istio-service-dashboard:
        gnetId: 7636
        revision: 146
        datasource: Prometheus
      istio-workload-dashboard:
        gnetId: 7630
        revision: 146
        datasource: Prometheus
      istio-performance-dashboard:
        gnetId: 11829
        revision: 146
        datasource: Prometheus
      istio-wasm-dashboard:
        gnetId: 13277
        revision: 103
        datasource: Prometheus

kube-state-metrics:
  # kube-state-metrics uses ServiceMonitor scraping rather than leveraging the Istio PodMonitor
  # aggregated scraping due to the fact that it exports metrics for other namespaces. The current
  # configuration of the Istio PodMonitor overrides the namespace label to always be "grafana-stack"
  # which is obviously not helpful.
  namespaceOverride: grafana-stack

prometheus-node-exporter:
  # The node-exporter is not injected with an Istio sidecar since it shares the network
  # namespace with the host. It will be monitored using its packaged ServiceMonitor.
  namespaceOverride: grafana-stack
