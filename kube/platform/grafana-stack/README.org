* Grafana Stack
** Prometheus Scraping
There are many different options when it comes to configuring Prometheus (or the "Prometheus-lite" Grafana agents) scraping when a service mesh is involved.

We take the approach described [[https://istio.io/latest/docs/ops/integrations/prometheus/#option-2-customized-scraping-configurations][here]], [[https://docs.google.com/document/d/1TTeN4MFmh4aUYYciR4oDBTtJsxl5-T5Tu3m3mGEdSo8/view][here]] and in the [[https://www.manning.com/books/istio-in-action][Istio in Action]] book where Istio injects Prometheus annotations that inform a PodMonitor on how to scrape the pod. The only difference between our approach and the approach described in these links is that we use the same PodMonitor to scrape both the Istio control plane and the data plan (yet to see why we wouldn't). The annotations look as follows:

#+begin_src yaml
  prometheus.io/scrape: "true"
  prometheus.io/path: /stats/prometheus
  prometheus.io/port: "15020"
#+end_src

The annotations injected by Istio will reference a port (typically 15020) and path (typically =/stats/prometheus=) that is handled by the sidecar to return Istio and Envoy metrics. If the Istio webhook notices that a pod already comes with these annotations it configures the sidecar to aggregate and return the application metrics and the Istio/Envoy metrics.

The single PodMonitor resource created by this package configures this scraping to occur across the entire mesh. Any workload injected with an Istio sidecar only needs to expose its application metrics and reference them via Prometheus annotations on its pods. The Istio sidecar injection process will handle the rest. No ServiceMonitors or additional PodMonitors need to be created in this case.

It's hard to find concrete advice on what should or shouldn't be injected within a cluster. But articles like the [[https://tanzu.vmware.com/developer/guides/service-routing-istio-refarch/][Tanzu Istio Reference Architecture]] give the impression that the =enableNamespacesByDefault= setting on the IstioOperator resource should be left as =false= and injection should be enabled on a per-namespace basis for namespaces that contain business services. Originally, I injected everything except for workloads like Prometheus and Grafana Agent which seem to recommend against injection; besides the auto-scaping benefits offered by the mesh PodMonitor described above this just generated a lot of noise (who really cares about Envoy metrics and traces for non-business workloads?) and made things harder to reason about in general.

Regarding the general recommendation not to inject Prometheus-type workloads, the [[https://istio.io/latest/docs/ops/integrations/prometheus/#tls-settings][Istio documentation]] mentions:

#+begin_quote
  However, the sidecar should not intercept requests for Prometheus because Prometheus’s model of direct endpoint access is incompatible with Istio’s sidecar proxy model.
#+end_quote

And [[https://superorbital.io/journal/istio-metrics-merging][this really good article]] on Istio monitoring with Prometheus mentions:

#+begin_quote
  Note: while you can still find people suggesting the use of an Istio sidecar being added to Prometheus to enable mTLS scraping, that approach is no longer recommended or necessary, do not do this! (read on for a full solution).
#+end_quote

In summary, all scraping of Istio-injected workloads is handled by a central PodMonitor that is reconciled by the default Grafana metrics agents which then remote write these metrics to a central Prometheus instance. All scraping of non-Istio-injected workloads is handled by ServiceMonitors that are also reconciled by the default Grafana metrics agents and remote written to the central Prometheus instance.

** Links
- https://discuss.istio.io/t/istio-mtls-and-pod-ip-port/7537/6
- https://superorbital.io/journal/istio-metrics-merging/
- https://docs.google.com/document/d/1TTeN4MFmh4aUYYciR4oDBTtJsxl5-T5Tu3m3mGEdSo8/view#
