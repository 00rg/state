---
# Source: loki/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: loki
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: loki/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    auth_enabled: false
    common:
      path_prefix: /var/loki
      replication_factor: 1
      storage:
        filesystem:
          chunks_directory: /var/loki/chunks
          rules_directory: /var/loki/rules
    limits_config:
      enforce_metric_name: false
      max_cache_freshness_per_query: 10m
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      split_queries_by_interval: 15m
    memberlist:
      join_members:
      - loki-memberlist
    query_range:
      align_queries_with_step: true
    schema_config:
      configs:
      - from: "2022-01-11"
        index:
          period: 24h
          prefix: loki_index_
        object_store: filesystem
        schema: v12
        store: boltdb-shipper
    server:
      grpc_listen_port: 9095
      http_listen_port: 3100
    storage_config:
      hedging:
        at: 250ms
        max_per_second: 20
        up_to: 3
---
# Source: loki/templates/service-memberlist.yaml
apiVersion: v1
kind: Service
metadata:
  name: loki-memberlist
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp
      port: 7946
      targetPort: http-memberlist
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/part-of: memberlist
---
# Source: loki/templates/single-binary/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: loki-headless
  namespace: grafana-stack
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
    variant: headless
    prometheus.io/service-monitor: "false"
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
---
# Source: loki/templates/single-binary/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: loki
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/component: single-binary
---
# Source: loki/templates/single-binary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: single-binary
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  serviceName: loki-headless
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: loki
      app.kubernetes.io/component: single-binary
  template:
    metadata:
      annotations:
        checksum/config: 87821bd5c189d36003eeacc677d8a3f4314dc02b2822375a19cb5ae4a16374de
        prometheus.io/path: /metrics
        prometheus.io/port: "3100"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: loki
        app.kubernetes.io/component: single-binary
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: loki
      automountServiceAccountToken: true
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: single-binary
          image: docker.io/grafana/loki:2.7.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=all
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: config
              mountPath: /etc/loki/config
            - name: storage
              mountPath: /var/loki
          resources:
            {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: loki
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/component: single-binary
              topologyKey: kubernetes.io/hostname
        
      volumes:
        - name: tmp
          emptyDir: {}
        - name: config
          configMap:
            name: loki
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "10Gi"
---
# Source: loki/templates/monitoring/prometheus-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
  name: loki-alerts
  namespace: grafana-stack
spec:
  groups:    
    - name: loki_alerts
      rules:
        - alert: LokiRequestErrors
          annotations:
            message: |
              {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}% errors.
          expr: |
            100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route)
              /
            sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
              > 10
          for: 15m
          labels:
            severity: critical
        - alert: LokiRequestPanics
          annotations:
            message: |
              {{ $labels.job }} is experiencing {{ printf "%.2f" $value }}% increase of panics.
          expr: |
            sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
          labels:
            severity: critical
        - alert: LokiRequestLatency
          annotations:
            message: |
              {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}s 99th percentile latency.
          expr: |
            namespace_job_route:loki_request_duration_seconds:99quantile{route!~"(?i).*tail.*"} > 1
          for: 15m
          labels:
            severity: critical
        - alert: LokiTooManyCompactorsRunning
          annotations:
            message: |
              {{ $labels.namespace }} has had {{ printf "%.0f" $value }} compactors running for more than 5m. Only one compactor should run at a time.
          expr: |
            sum(loki_boltdb_shipper_compactor_running) by (namespace) > 1
          for: 5m
          labels:
            severity: warning
    - name: loki_canaries_alerts
      rules:
        - alert: LokiCanaryLatency
          annotations:
            message: |
              {{ $labels.job }} is experiencing {{ printf "%.2f" $value }}s 99th percentile latency.
          expr: |
            histogram_quantile(0.99, sum(rate(loki_canary_response_latency_seconds_bucket[5m])) by (le, namespace, cluster)) > 5
          for: 15m
          labels:
            severity: warning
---
# Source: loki/templates/monitoring/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
  name: loki-rules
  namespace: grafana-stack
spec:
  groups:    
    - name: loki_rules
      rules:
        - expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job))
          labels:
            cluster: loki
          record: job:loki_request_duration_seconds:99quantile
        - expr: histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job))
          labels:
            cluster: loki
          record: job:loki_request_duration_seconds:50quantile
        - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (job) / sum(rate(loki_request_duration_seconds_count[1m]))
            by (job)
          labels:
            cluster: loki
          record: job:loki_request_duration_seconds:avg
        - expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)
          labels:
            cluster: loki
          record: job:loki_request_duration_seconds_bucket:sum_rate
        - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (job)
          labels:
            cluster: loki
          record: job:loki_request_duration_seconds_sum:sum_rate
        - expr: sum(rate(loki_request_duration_seconds_count[1m])) by (job)
          labels:
            cluster: loki
          record: job:loki_request_duration_seconds_count:sum_rate
        - expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job, route))
          labels:
            cluster: loki
          record: job_route:loki_request_duration_seconds:99quantile
        - expr: histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job, route))
          labels:
            cluster: loki
          record: job_route:loki_request_duration_seconds:50quantile
        - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (job, route) / sum(rate(loki_request_duration_seconds_count[1m]))
            by (job, route)
          labels:
            cluster: loki
          record: job_route:loki_request_duration_seconds:avg
        - expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)
          labels:
            cluster: loki
          record: job_route:loki_request_duration_seconds_bucket:sum_rate
        - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (job, route)
          labels:
            cluster: loki
          record: job_route:loki_request_duration_seconds_sum:sum_rate
        - expr: sum(rate(loki_request_duration_seconds_count[1m])) by (job, route)
          labels:
            cluster: loki
          record: job_route:loki_request_duration_seconds_count:sum_rate
        - expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, namespace, job, route))
          labels:
            cluster: loki
          record: namespace_job_route:loki_request_duration_seconds:99quantile
        - expr: histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, namespace, job, route))
          labels:
            cluster: loki
          record: namespace_job_route:loki_request_duration_seconds:50quantile
        - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (namespace, job, route)
            / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
          labels:
            cluster: loki
          record: namespace_job_route:loki_request_duration_seconds:avg
        - expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, namespace, job,
            route)
          labels:
            cluster: loki
          record: namespace_job_route:loki_request_duration_seconds_bucket:sum_rate
        - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (namespace, job, route)
          labels:
            cluster: loki
          record: namespace_job_route:loki_request_duration_seconds_sum:sum_rate
        - expr: sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
          labels:
            cluster: loki
          record: namespace_job_route:loki_request_duration_seconds_count:sum_rate
---
# Source: loki/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: loki
  labels:
    helm.sh/chart: loki-3.8.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: loki
    app.kubernetes.io/version: "2.7.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: loki
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      path: /metrics
      relabelings:
        - sourceLabels: [job]
          replacement: "grafana-stack/$1"
          targetLabel: job
        - replacement: "loki"
          targetLabel: cluster
      scheme: http
